{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNN)\n",
    "\n",
    "**Artificial Inteligence (AI)** makes it possible to learn from experience in algorithmic, direct-forwarded way, adjust to new inputs and perform human-like tasks. Most AI examples that are heared about today – from chess-playing computers to self-driving cars – rely heavily on deep learning and natural language processing. One of the key AI tasks applied nowadays - image recognition and object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Neural Networks** (NNs) are computational processing systems of which are heavily inspired by way biological nervous systems (such as the human brain) operate. NNs are mainly comprised of a high number of interconnected computational nodes (referred to as neurons), of which work entwine in a distributed fashion to collectively learn from the input in order to optimise its final output.\n",
    "\n",
    "![nn](https://www.researchgate.net/publication/341311377/figure/fig1/AS:890217116487682@1589255708435/A-simple-feedforward-NN-with-one-hidden-layer-Figure-from-2-Figure-2-The-structure.ppm)\n",
    "\n",
    "Fig. 1: A simple three layered feedforward neural network (FNN), comprised of a input layer, a hidden layer and an output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neureal Network\n",
    "\n",
    "A **Convolutional Neural Network** (CNN) is a Deep Learning algorithm that can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image, and be able to separate one from the other. The pre-processing required in a CNN is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, CNNs have the ability to learn these filters/characteristics.\n",
    "\n",
    "![cnn](https://saturncloud.io/images/blog/a-cnn-sequence-to-classify-handwritten-digits.webp)\n",
    "\n",
    "Fig. 2: A CNN general structure for classifying handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input image\n",
    "\n",
    "As an input, CNN expects an RGB image that has been separated by its three color planes — Red, Green, and Blue. There are a number of such color spaces in which images exist — Grayscale, RGB, HSV, CMYK, etc.\n",
    "\n",
    "![cnn_input](https://saturncloud.io/images/blog/4x4x3-rgb-image.webp)\n",
    "\n",
    "Fig. 3: Input image 4x4x3 RGB\n",
    "\n",
    "Imagine how computationally intensive things would get once the images reach dimensions, say 8K (7680×4320). More pixels require more parameters to train. The role of CNN is to reduce the images into a form that is easier to process, without losing features that are critical for getting a good prediction. This is important to design an architecture that is not only good at learning features but also scalable to massive datasets. So, traditional NN works not with original image, but with smalled representation of image, with all important features presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN architecture\n",
    "\n",
    "CNN architecture (Fig. 2) is typically composed of three types of layers (or building blocks): convolution, pooling, and fully connected layers, known as simple NN. When these layers are stacked, a CNN architecture has been formed.\n",
    "\n",
    "### Convolution layer\n",
    "\n",
    "A convolution layer is a fundamental component of the CNN architecture that performs feature extraction, which typically consists of a combination of linear and nonlinear operations, i.e., convolution operation and activation function. The term [convolution](https://timdettmers.com/2015/03/26/convolution-deep-learning/) refers to the mathematical combination of two functions to produce a third function.\n",
    "\n",
    "In the case of a CNN, the convolution is performed on the input data with the use of a **filter** or **kernel** (these terms are used interchangeably) to then produce a **feature map**. The filter moves to the right with a certain **stride** till it parses the complete width. Moving on, it hops down to the beginning (left) of the image with the same Stride and repeats the process until the entire image is traversed.\n",
    "\n",
    "![cnn_conv](https://saturncloud.io/images/blog/convolution-operation-on-a-mxnx3-image-matrix-with-a-3x3x3-kernel.gif)\n",
    "\n",
    "Fig. 4: Convolution operation on a MxNx3 image matrix with a 3x3x3 Kernel and Stride 1\n",
    "\n",
    "The objective of the Convolution operation is to extract the high-level features such as edges, from the input image. CNNs need not be limited to only one Convolutional Layer. As shown below, different filters extract different edges (like vertical or horizontal).\n",
    "\n",
    "![cnn_conv_lenna](https://miro.medium.com/v2/resize:fit:828/format:webp/1*350OTygt6JoJk54GNnZLSQ.png)\n",
    "\n",
    "Fig. 5: Result of convolution for horizontal and vertical filters\n",
    "\n",
    "Conventionally, the first Convolutional layer is responsible for capturing the low-level features such as edges, color, gradient orientation, etc. With added layers, the architecture adapts to the high-level features as well, giving a network that has a wholesome understanding of images in the dataset, similar to how we would.\n",
    "\n",
    "As described later, the process of training a CNN model with regard to the convolution layer is to identify the kernels that work best for a given task based on a given training dataset. Kernels are the only parameters automatically learned during the training process in the convolution layer, on the other hand, the size of the kernels, number of kernels, padding, and stride are hyperparameters that need to be set before the training process starts.\n",
    "\n",
    "### Activation function\n",
    "\n",
    "The outputs of a linear operation such as convolution are then passed through a nonlinear activation function. The most common nonlinear activation function used presently is the rectified linear unit (ReLU), which simply computes the function: \n",
    "\n",
    "f(x) = max(0, x)\n",
    "\n",
    "The reason, why other functions not used, like sigmoid or hyperbolic tangent (tanh) is simple: there are no pixel color representation for function output negative value. In general, ReLU just converts negative values to 0, while positive remains the same.\n",
    "\n",
    "![cnn_act_func](https://media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs13244-018-0639-9/MediaObjects/13244_2018_639_Fig5_HTML.png?as=webp)\n",
    "\n",
    "Fig. 6: Commonly applied activation functions - (a) rectified linear unit (ReLU), (b) sigmoid, and (c) hyperbolic tangent (tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling layer\n",
    "\n",
    "A pooling layer provides a typical downsampling operation which reduces general image size, or, in other words, count of the feature maps in order to introduce a translation invariance to small shifts and distortions, and decrease the number of subsequent learnable parameters. It is important, that there is no learnable parameter in any of the pooling layers, whereas filter size, stride, and padding are hyperparameters in pooling operations, similar to convolution operations.\n",
    "\n",
    "### Max pooling\n",
    "\n",
    "The most popular form of pooling operation is max pooling, which extracts patches from the input feature maps, outputs the maximum value in each patch, and discards all the other values (Fig. 7). A max pooling with a filter of size 2 × 2 with a stride of 2 is commonly used in practice. This downsamples the in-plane dimension of feature maps by a factor of 2.\n",
    "\n",
    "### Average pooling\n",
    "\n",
    "Another commonly used pooling operation is a global average pooling. A global average pooling performs an extreme type of downsampling, where a feature map with size of height × width is downsampled into a 1 × 1 array by simply taking the average of all the elements in each feature map, whereas the depth of feature maps is retained. This operation is typically applied only once before the fully connected layers. The advantages of applying global average pooling are as follows: (1) reduces the number of learnable parameters, (2) enables the CNN to accept inputs of variable size and (3) noise suppression, but only in some certain cases, which are out of scope of this article.\n",
    "\n",
    "![cnn_pooling](https://saturncloud.io/images/blog/types-of-pooling.webp)\n",
    "\n",
    "Fig 7: Different types of pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fully-connected layer\n",
    "\n",
    "The fully-connected layer contains neurons of which are directly connected to the neurons in the two adjacent layers, without being connected to any layers within them. This is analogous to way that neurons are arranged in traditional forms of NN. (Fig 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Building CNN for recognizing hand signs using MNIST dataset\n",
    "\n",
    "Let's try to use all knowledge base described before to build CNN for sign recognition. It is a well-known multi-class classification problem. We will be using the [Sign Language MNIST](https://www.kaggle.com/datasets/datamunge/sign-language-mnist) dataset, which contains 28x28 images of hands depicting the 26 letters of the english alphabet. Please, fell free to check, look and investigate the original data.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "### Data preparations\n",
    "\n",
    "At first, let's import all required libs. We will be using NumPy + TensorFlow as a general NN framework. Also, let's pick pyplot for data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the training and test sets (the test set will actually be used as a validation set). If you have any problems, use direct link [here](https://www.kaggle.com/datasets/datamunge/sign-language-mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sign_mnist_train.csv\n",
    "!gdown --id 1z0DkA9BytlLxO1C0BAWzknLyQmZAp0HR\n",
    "# sign_mnist_test.csv\n",
    "!gdown --id 1z1BIj4qmri59GWBG4ivMNFtpZ4AXIbzg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some globals with the path to both files you just downloaded and look at how the data looks like within the `csv` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FILE = './sign_mnist_train.csv'\n",
    "VALIDATION_FILE = './sign_mnist_test.csv'\n",
    "\n",
    "with open(TRAINING_FILE) as training_file:\n",
    "  line = training_file.readline()\n",
    "  print(f\"First line (header) looks like this:\\n{line}\")\n",
    "  line = training_file.readline()\n",
    "  print(f\"Each subsequent line (data points) look like this:\\n{line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each file includes a header (the first line) and each subsequent data point is represented as a line that contains 785 values.\n",
    "\n",
    "The first value is the label (the numeric representation of each letter) and the other 784 values are the value of each pixel of the image. Remember that the original images have a resolution of 28x28, which sums up to 784 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the dataset\n",
    "\n",
    " Now let's define function `parse_data_from_input` below.\n",
    "\n",
    " This function should be able to read a file passed as input and return 2 numpy arrays, one containing the labels and one containing the 28x28 representation of each image within the file. These numpy arrays should have type `float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_from_input(filename):\n",
    "  \"\"\"\n",
    "  Parses the images and labels from a CSV file\n",
    "\n",
    "  Args:\n",
    "    filename (string): path to the CSV file\n",
    "\n",
    "  Returns:\n",
    "    images, labels: tuple of numpy arrays containing the images and labels\n",
    "  \"\"\"\n",
    "\n",
    "  # Use csv.reader, passing in the appropriate delimiter\n",
    "  # Remember that csv.reader can be iterated and returns one line in each iteration\n",
    "  arr = np.loadtxt(filename, delimiter=',', skiprows=1)\n",
    "\n",
    "  labels = np.transpose(arr[:,0])\n",
    "  images = arr[:, 1:].reshape((arr[:, 1:].shape[0], 28, 28))\n",
    "\n",
    "  return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function\n",
    "training_images, training_labels = parse_data_from_input(TRAINING_FILE)\n",
    "validation_images, validation_labels = parse_data_from_input(VALIDATION_FILE)\n",
    "\n",
    "print(f\"Training images has shape: {training_images.shape} and dtype: {training_images.dtype}\")\n",
    "print(f\"Training labels has shape: {training_labels.shape} and dtype: {training_labels.dtype}\")\n",
    "print(f\"Validation images has shape: {validation_images.shape} and dtype: {validation_images.dtype}\")\n",
    "print(f\"Validation labels has shape: {validation_labels.shape} and dtype: {validation_labels.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "Training images has shape: (27455, 28, 28) and dtype: float64\n",
    "Training labels has shape: (27455,) and dtype: float64\n",
    "Validation images has shape: (7172, 28, 28) and dtype: float64\n",
    "Validation labels has shape: (7172,) and dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the numpy arrays\n",
    "\n",
    "Now that you have converted the initial csv data into a format that is compatible with computer vision tasks, take a moment to actually see how the images of the dataset look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a sample of 10 images from the training set\n",
    "def plot_categories(training_images, training_labels):\n",
    "  fig, axes = plt.subplots(1, 10, figsize=(16, 15))\n",
    "  axes = axes.flatten()\n",
    "  letters = list(string.ascii_lowercase)\n",
    "\n",
    "  for k in range(10):\n",
    "    img = training_images[k]\n",
    "    img = np.expand_dims(img, axis=-1)\n",
    "    img = array_to_img(img)\n",
    "    ax = axes[k]\n",
    "    ax.imshow(img, cmap=\"Greys_r\")\n",
    "    ax.set_title(f\"{letters[int(training_labels[k])]}\")\n",
    "    ax.set_axis_off()\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "plot_categories(training_images, training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the generators for the CNN\n",
    "\n",
    "Now that it has successfully organized the data in a way that can be easily fed to Keras' `ImageDataGenerator`, it is time for you to code the generators that will yield batches of images, both for training and validation. For this complete the `train_val_generators` function below.\n",
    "\n",
    "Some important notes:\n",
    "\n",
    "- The images in this dataset come in the same resolution so we don't need to set a custom `target_size` in this case. In fact, you can't even do so because this time you will not be using the `flow_from_directory` method (as in previous assignments). Instead you will use the [`flow`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow) method.\n",
    "- We need to add the \"color\" dimension to the numpy arrays that encode the images. These are black and white images, so this new dimension should have a size of 1 (instead of 3, which is used when dealing with colored images). Take a look at the function [`np.expand_dims`](https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_generators(training_images, training_labels, validation_images, validation_labels):\n",
    "  \"\"\"\n",
    "  Creates the training and validation data generators\n",
    "\n",
    "  Args:\n",
    "    training_images (array): parsed images from the train CSV file\n",
    "    training_labels (array): parsed labels from the train CSV file\n",
    "    validation_images (array): parsed images from the test CSV file\n",
    "    validation_labels (array): parsed labels from the test CSV file\n",
    "\n",
    "  Returns:\n",
    "    train_generator, validation_generator - tuple containing the generators\n",
    "  \"\"\"\n",
    "\n",
    "  # In this section you will have to add another dimension to the data\n",
    "  # So, for example, if your array is (10000, 28, 28)\n",
    "  # You will need to make it (10000, 28, 28, 1)\n",
    "  # Hint: np.expand_dims\n",
    "  training_images = np.expand_dims(training_images, axis=3)\n",
    "  validation_images = np.expand_dims(validation_images, axis=3)\n",
    "\n",
    "  # Instantiate the ImageDataGenerator class\n",
    "  # Don't forget to normalize pixel values\n",
    "  # and set arguments to augment the images (if desired)\n",
    "  train_datagen = ImageDataGenerator(rescale = 1./255.,\n",
    "                                   rotation_range = 40,\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "\n",
    "  # Pass in the appropriate arguments to the flow method\n",
    "  train_generator = train_datagen.flow(x=training_images,\n",
    "                                       y=tf.keras.utils.to_categorical(training_labels, num_classes=26),\n",
    "                                       batch_size=32)\n",
    "\n",
    "\n",
    "  # Instantiate the ImageDataGenerator class (don't forget to set the rescale argument)\n",
    "  # Remember that validation data should not be augmented\n",
    "  validation_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
    "\n",
    "  # Pass in the appropriate arguments to the flow method\n",
    "  validation_generator = validation_datagen.flow(x=validation_images,\n",
    "                                                 y=tf.keras.utils.to_categorical(validation_labels, num_classes=26),\n",
    "                                                 batch_size=32)\n",
    "\n",
    "  return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your generators\n",
    "train_generator, validation_generator = train_val_generators(training_images, training_labels, validation_images, validation_labels)\n",
    "\n",
    "print(f\"Images of training generator have shape: {train_generator.x.shape}\")\n",
    "print(f\"Labels of training generator have shape: {train_generator.y.shape}\")\n",
    "print(f\"Images of validation generator have shape: {validation_generator.x.shape}\")\n",
    "print(f\"Labels of validation generator have shape: {validation_generator.y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "Images of training generator have shape: (27455, 28, 28, 1)\n",
    "Labels of training generator have shape: (27455,)\n",
    "Images of validation generator have shape: (7172, 28, 28, 1)\n",
    "Labels of validation generator have shape: (7172,)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding the CNN\n",
    "\n",
    "One last step before training is to define the architecture of the model that will be trained.\n",
    "\n",
    "Below provided `create_model` function. This function should return a Keras model that uses the `Sequential` or the `Functional` API. The last layer of this model should have a number of units that corresponds to the number of possible categories, as well as the correct activation function. Aside from defining the architecture of the model, you should also compile it so make sure to use a `loss` function that is suitable for multi-class classification.\n",
    "\n",
    "**It is better use no more than 2 Conv2D and 2 MaxPooling2D layers to achieve the desired performance. Feel free for your own experiments!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  \n",
    "  # Define the model\n",
    "  # Use no more than 2 Conv2D and 2 MaxPooling2D\n",
    "  model = tf.keras.models.Sequential([\n",
    "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
    "    # This is the first convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fourth convolution\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(26, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "  model.compile(optimizer = 'rmsprop',\n",
    "                loss = 'categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  ### END CODE HERE\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model\n",
    "model = create_model()\n",
    "\n",
    "# Train your model\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=15,\n",
    "                    validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a look at your training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the chart for accuracy and loss on both training and validation\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Note**: All provided hyperparameteres for training were discovered using empirical research. Feel free to experiment with different NN layers, epochs number, activation function, filter size or count. Try to experiment with model's architecture or the augmentation techniques to see if you can achieve higher levels of accuracy. A reasonable benchmark is to achieve over 99% accuracy for training and over 95% accuracy for validation within 15 epochs._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Keiron O’Shea and Ryan Nash: An Introduction to Convolutional Neural Networks [link](https://arxiv.org/pdf/1511.08458.pdf). \n",
    "2. Niketh Narasimhan: Convolutional Neural Networks-An Intuitive approach [link](https://medium.com/analytics-vidhya/convolutional-neural-networks-an-intuitive-approach-part-2-729bfb5e4d87)\n",
    "3. Rikiya Yamashita: Convolutional neural networks: an overview and application in radiology [link](https://insightsimaging.springeropen.com/articles/10.1007/s13244-018-0639-9)\n",
    "4. Laurence Moroney and Andrew Ng: Convolutional Neural Networks in TensorFlow [link](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/)\n",
    "5. Tim Dettmers: Understanding Convolution in Deep Learning [link](https://timdettmers.com/2015/03/26/convolution-deep-learning/)\n",
    "6. Sumit Saha: A Guide to Convolutional Neural Networks — the ELI5 way [link](https://saturncloud.io/blog/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
